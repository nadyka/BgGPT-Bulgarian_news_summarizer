Based on the directory tree provided, it seems like this workspace is related to a project focused on summarizing Bulgarian news articles. Let's break down the components and try to understand what this workspace does:

1. **Project Description**:
   - The project appears to be a Bulgarian news summarizer, as indicated by the directory name "BgGPT-Bulgarian_news_summarizer."
   - The project seems to involve processing and summarizing news articles in Bulgarian.

2. **Directory Structure**:
   - The project contains multiple directories named with timestamps, each containing multiple `.txt` files with news article titles.
   - There are also directories related to articles and databases, such as `articles`, `articles.db`, and `articles_batch_2`.
   - Additionally, there are Python scripts like `Adding_3_most_read_articles.py`, `BgGPT_chat_instructions.py`, `Final_project_main_functions_summary_sidebar_calendar.py`, etc., which likely contain the code for various functionalities of the project.

3. **Main Technologies and Tools**:
   - The project seems to involve Natural Language Processing (NLP) techniques for text summarization, as indicated by files like `Main_functions_top_10_article_summaries.py` and `Main-sidebar_functions_and_NLTK_to_summarize.py`.
   - There are references to LMstudio, possibly indicating the use of a language model for text processing.
   - The presence of `streamlit` related files suggests the possibility of a web-based interface for interacting with the summarization tool.

4. **Purpose**:
   - The project likely aims to provide a tool for summarizing Bulgarian news articles automatically.
   - It may help users quickly grasp the key points of news articles without reading the entire content.

5. **Additional Information**:
   - There are Jupyter notebooks like `Final_project_steps.ipynb` and `ChatGPT_project_reqs.ipynb`, which might contain project requirements, steps, or documentation.
   - The presence of a `LICENSE` file indicates that the project is open source and may have specific licensing terms.

In conclusion, this workspace appears to be a project focused on developing a Bulgarian news summarization tool using NLP techniques, possibly leveraging LMstudio and streamlit for implementation. The project aims to automate the process of summarizing news articles for easier consumption by users. Further details can be found in the code files and documentation within the directories.
# The user is selecting lines 1 to 181 of the c:\Users\Nadia Stoyanova\Documents\GitHub\BgGPT-Bulgarian_news_summarizer\Final_project_main_functions_summary_sidebar_calendar_HTML.py file, which is in the python language.

```
1: import requests
2: from bs4 import BeautifulSoup
3: import streamlit as st
4: import pandas as pd
5: import sqlite3
6: import datetime
7: import os
8: import re  # Import regex
9: import random
10: from random import randint
11: from nltk.tokenize import word_tokenize, sent_tokenize
12: import nltk
13: 
14: # Download necessary NLTK resources
15: nltk.download('punkt')
16: 
17: # Custom Bulgarian stopwords list
18: bulgarian_stopwords = set([
19:     "и", "в", "на", "с", "за", "не"
20: ])
21: 
22: # Constants
23: BASE_URL = "https://www.mediapool.bg/"
24: DATABASE_NAME = 'articles.db'
25: FOLDER_NAME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
26: os.makedirs(FOLDER_NAME, exist_ok=True)
27: 
28: def sanitize_title(title):
29:     return re.sub(r'[\\/*?:"<>|]', '', title)[:50]
30: 
31: def setup_database():
32:     with sqlite3.connect(DATABASE_NAME) as conn:
33:         conn.execute('''
34:             CREATE TABLE IF NOT EXISTS articles (
35:                 id INTEGER PRIMARY KEY AUTOINCREMENT,
36:                 title TEXT NOT NULL,
37:                 link TEXT NOT NULL,
38:                 summary TEXT,
39:                 views INTEGER DEFAULT 0,
40:                 retrieval_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
41:             )
42:         ''')
43: 
44: 
45: # Fetch and store news articles in a local file.
46: def fetch_news(articles_folderpath, browser):
47:     #Define the user agents
48:     user_agents = {
49:         'firefox': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0',
50:         'chrome': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36',
51:         'edge': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.902.78 Safari/537.36 Edg/92.0.902.78',
52:         'safari': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15',
53:         'opera': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36 OPR/64.0.3417.83'
54:     }
55:     headers = {'User-Agent': user_agents.get(browser, 'Mozilla/5.0')}
56:     # Set up path to save each article to file
57:     articles_folderpath = os.getcwd() + '\\' + articles_folderpath
58:     #base_news_folder = r'C:\Users\Nadia Stoyanova\Documents\GitHub\BgGPT-Bulgarian_news_summarizer\articles'
59:     file2save = 'response.html'
60:     article_filepath = articles_folderpath + '\\' + file2save
61: 
62:     # request the news page    
63:     try:
64:         response = requests.get(BASE_URL, headers=headers)
65:         if response.status_code == 200:
66:             print (f"response.status_code: {response.status_code}")
67:             # Save the response content as HTML            
68:             save_html(response, article_filepath)
69:             #with open(article_filepath, 'w', encoding='utf-8') as f:
70:     except Exception as e:
71:         print(f"Error fetching news: {e}")        
72: 
73: # def make_soup_from_news_html(article_filepath)
74: #     if response is not None:
75: #         soup = parse_and_save_soup(response, soup_filepath)
76: #         if soup is not None:
77: #             save_articles(soup)
78: #     return response        
79: #         #f.write(response.text)
80: #         #print (f"#response.html successfully written out")
81: #         #response.html successfully written out            
82: 
83: 
84: def save_html(response, filepath):
85:     try:
86:         with open(filepath, 'w', encoding='utf-8') as f:
87:             f.write(response.text)
88:             print (f"#response.html successfully written out")
89:     except Exception as e:
90:         print(f"Error writing HTML file: {e}")
91: 
92:     #TODO print return value is successful - fetch and save news response was successful
93: 
94: 
95: def parse_and_save_soup(response, soup_filepath):
96:     try:
97:         soup = BeautifulSoup(response.text, 'html.parser')
98:         with open(soup_filepath, 'w', encoding='utf-8') as f:
99:             f.write(soup.get_text())
100:             print (f"soup content written out from soup.get_text(): {soup.get_text()}")
101:         return soup
102:     except Exception as e:
103:         print(f"Error writing soup file: {e}")
104:         return None
105: 
106: def save_articles_to_db(soup):
107:     try:
108:         news_items = soup.find_all('article', limit=10)
109:         with sqlite3.connect(DATABASE_NAME) as conn:
110:             conn.execute("DELETE FROM articles WHERE date(retrieval_date) = date('now')")
111:             for item in news_items:
112:                 a_tag = item.find('a')
113:                 if a_tag and a_tag['href']:
114:                     title, summary = extract_title_and_summary(a_tag['href'])
115:                     views = randint(1, 100)
116:                     conn.execute("INSERT INTO articles (title, link, summary, views, retrieval_date) VALUES (?, ?, ?, ?, ?)",
117:                                  (title, a_tag['href'], summary, views, datetime.datetime.now()))
118:             conn.commit()
119:     except requests.RequestException as e:
120:         st.error(f"Exception during raw data news_fetch_and_save article_file: {e}")
121: 
122: def extract_title_and_summary(url):
123:     headers = {'User-Agent': 'Mozilla/5.0'}
124:     try:
125:         response = requests.get(url, headers=headers)
126:         if response.ok:
127:             soup = BeautifulSoup(response.text, 'html.parser')
128:             title = soup.find('title').text if soup.find('title') else "No title found"
129:             article_text = ' '.join([p.text for p in soup.find_all('p')])
130:             sentences = sent_tokenize(article_text)
131:             summary = sentences[0][:100] if sentences else "No summary available"
132:             tokens = word_tokenize(article_text)  # Tokenize the article text
133:             return title, summary
134:     except Exception as e:
135:         return "Failed to extract", str(e)
136: 
137: def get_popular_articles(selected_date):
138:     with sqlite3.connect(DATABASE_NAME) as conn:
139:         return pd.read_sql("""
140:             SELECT title, link, summary
141:             FROM articles
142:             WHERE date(retrieval_date) = date(?)
143:             ORDER BY views DESC
144:             LIMIT 5
145:         """, conn, params=(selected_date,))
146: 
147: def display_articles():
148:     st.title("TOP 10 News Summary from MediaPool")
149:     setup_database()
150: 
151:     selected_date = st.sidebar.date_input("Select a date", datetime.date.today())
152:     if st.sidebar.button("Fetch News Now"):
153:         articles_folderpath = 'articles'
154:         soup_filepath = 'response_soup.txt'
155:         response = fetch_news(articles_folderpath)
156:         if response is not None:
157:             soup = parse_and_save_soup(response, soup_filepath)
158:             if soup is not None:
159:                 save_articles(soup)
160: 
161:     popular_articles = get_popular_articles(selected_date)
162:     if not popular_articles.empty:
163:         st.sidebar.write(f"Most Popular Articles for {selected_date}:")
164:         for index, row in popular_articles.iterrows():
165:             st.sidebar.write(f"{row['title']} - {row['link']}")
166: 
167:     # Display today's articles
168:     with sqlite3.connect(DATABASE_NAME) as conn:
169:         todays_articles = pd.read_sql("SELECT title, link, summary FROM articles WHERE date(retrieval_date) = date('now')", conn)
170:     if not todays_articles.empty:
171:         todays_articles.index = range(1, len(todays_articles) + 1)
172:         for _, row in todays_articles.iterrows():
173:             with st.expander(f"{row['title']}"):
174:                 st.write(f"Накратко: {row['summary']}")
175:                 st.write(f"Прочетете цялата статия: [link]({row['link']})")
176:     else:
177:         st.write("Не са намерени статии.")
178: 
179: if __name__ == "__main__":
180:     display_articles()
181: 
```


Linter errors in this selection:
Lines 159-159: Diagnostic message: Pylance(reportUndefinedVariable https://github.com/microsoft/pyright/blob/main/docs/configuration.md#reportUndefinedVariable) Warning: "save_articles" is not defined


Lines 9-9: Diagnostic message: Pylance(undefined) Hint: "random" is not accessed


Lines 132-132: Diagnostic message: Pylance(undefined) Hint: "tokens" is not accessed


Lines 164-164: Diagnostic message: Pylance(undefined) Hint: "index" is not accessed



# The user is on a Windows machine.

# The last command and its output in the terminal is: `
r\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_summarizer\\C:\\Users\\Nadia Stoyanova\\Documents\\GitHub\\BgGPT-Bulgarian_news_sum
`
# The current project is a git repository on branch: main
# The following files have been changed since the last commit: articles.db
# Here is the detailed git diff: diff --git a/articles.db b/articles.db
index 08cee87..c8f594e 100644
Binary files a/articles.db and b/articles.db differ

